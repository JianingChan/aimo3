{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af4cb9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "\n",
    "from vllm import LLM, SamplingParams\n",
    "from vllm.engine.arg_utils import AsyncEngineArgs\n",
    "from vllm.sampling_params import RequestOutputKind\n",
    "from vllm.v1.engine.async_llm import AsyncLLM\n",
    "\n",
    "\n",
    "def call_vllm_api(\n",
    "    engine: AsyncLLM, \n",
    "    prompt: str, \n",
    "    request_id: str,\n",
    "    verbose: bool = True,\n",
    ") -> str | None:\n",
    "\n",
    "    sampling_params = SamplingParams(\n",
    "                temperature=0.1,              # randomness of the sampling\n",
    "                min_p=0.05,\n",
    "                top_p=0.9,\n",
    "                skip_special_tokens=True,     # Whether to skip special tokens in the output\n",
    "                max_tokens=32768 ,\n",
    "                # stop=[\"</think>\"]\n",
    "            )\n",
    "    try:\n",
    "        async for output in engine.generate(\n",
    "            request_id=request_id, prompt=prompt, sampling_params=sampling_params\n",
    "        ):\n",
    "            # Process each completion in the output\n",
    "            for completion in output.outputs:\n",
    "                # In DELTA mode, we get only new tokens generated since last iteration\n",
    "                new_text = completion.text\n",
    "                if new_text:\n",
    "                    print(new_text, end=\"\", flush=True)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå Error during streaming: {e}\")\n",
    "        raise\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
