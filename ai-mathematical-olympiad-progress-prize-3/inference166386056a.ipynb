{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-24T05:26:00.768713Z",
     "iopub.status.busy": "2025-11-24T05:26:00.768560Z",
     "iopub.status.idle": "2025-11-24T05:26:27.215815Z",
     "shell.execute_reply": "2025-11-24T05:26:27.215361Z",
     "shell.execute_reply.started": "2025-11-24T05:26:00.768696Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "# assert torch.__version__ == \"2.8.0+cu128\", (f\"Torch version is {torch.__version__} instead of 2.8.0+cu128\")\n",
    "assert torch.cuda.is_available and torch.cuda.device_count() == 1, \"GPU not enabled\"\n",
    "import numpy as np\n",
    "# assert np.__version__ == \"2.2.0\", (f\"Numpy version is {np.__version__} instead of 2.2.0\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-24T05:26:27.216558Z",
     "iopub.status.busy": "2025-11-24T05:26:27.216341Z",
     "iopub.status.idle": "2025-11-24T05:26:27.219075Z",
     "shell.execute_reply": "2025-11-24T05:26:27.218697Z",
     "shell.execute_reply.started": "2025-11-24T05:26:27.216547Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TRANSFORMERS_NO_TF\"] = \"1\"\n",
    "os.environ[\"TRANSFORMERS_NO_FLAX\"] = \"1\"\n",
    "os.environ[\"TRITON_PTXAS_PATH\"] = \"/usr/local/cuda/bin/ptxas\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jianing/anaconda3/envs/kag2/bin/python\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.execute_input": "2025-11-24T05:26:27.219872Z",
     "iopub.status.busy": "2025-11-24T05:26:27.219744Z",
     "iopub.status.idle": "2025-11-24T05:26:27.939668Z",
     "shell.execute_reply": "2025-11-24T05:26:27.939237Z",
     "shell.execute_reply.started": "2025-11-24T05:26:27.219862Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "# import kaggle_evaluation.aimo_3_inference_server\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "start_time = time.time()\n",
    "cutoff_time = start_time + (4 * 60 + 53) * 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.execute_input": "2025-11-24T05:26:27.940407Z",
     "iopub.status.busy": "2025-11-24T05:26:27.940185Z",
     "iopub.status.idle": "2025-11-24T05:27:37.771229Z",
     "shell.execute_reply": "2025-11-24T05:27:37.770606Z",
     "shell.execute_reply.started": "2025-11-24T05:26:27.940395Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-27 23:04:32 [utils.py:253] non-default args: {'dtype': 'bfloat16', 'seed': 391, 'max_model_len': 32768, 'enable_prefix_caching': True, 'max_num_seqs': 2, 'disable_log_stats': True, 'model': '/mnt/d/Qwen3-1.7B'}\n",
      "INFO 11-27 23:04:33 [model.py:631] Resolved architecture: Qwen3ForCausalLM\n",
      "INFO 11-27 23:04:33 [model.py:1745] Using max model len 32768\n",
      "INFO 11-27 23:04:33 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=103695)\u001b[0;0m INFO 11-27 23:04:33 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='/mnt/d/Qwen3-1.7B', speculative_config=None, tokenizer='/mnt/d/Qwen3-1.7B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=391, served_model_name=/mnt/d/Qwen3-1.7B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 4, 'local_cache_dir': None}\n",
      "\u001b[1;36m(EngineCore_DP0 pid=103695)\u001b[0;0m INFO 11-27 23:04:35 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://172.22.6.29:58625 backend=nccl\n",
      "\u001b[1;36m(EngineCore_DP0 pid=103695)\u001b[0;0m INFO 11-27 23:04:35 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "\u001b[1;36m(EngineCore_DP0 pid=103695)\u001b[0;0m ERROR 11-27 23:04:35 [core.py:842] EngineCore failed to start.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=103695)\u001b[0;0m ERROR 11-27 23:04:35 [core.py:842] Traceback (most recent call last):\n",
      "\u001b[1;36m(EngineCore_DP0 pid=103695)\u001b[0;0m ERROR 11-27 23:04:35 [core.py:842]   File \"/home/jianing/anaconda3/envs/kag2/lib/python3.12/site-packages/vllm/v1/engine/core.py\", line 833, in run_engine_core\n",
      "\u001b[1;36m(EngineCore_DP0 pid=103695)\u001b[0;0m ERROR 11-27 23:04:35 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=103695)\u001b[0;0m ERROR 11-27 23:04:35 [core.py:842]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(EngineCore_DP0 pid=103695)\u001b[0;0m ERROR 11-27 23:04:35 [core.py:842]   File \"/home/jianing/anaconda3/envs/kag2/lib/python3.12/site-packages/vllm/v1/engine/core.py\", line 606, in __init__\n",
      "\u001b[1;36m(EngineCore_DP0 pid=103695)\u001b[0;0m ERROR 11-27 23:04:35 [core.py:842]     super().__init__(\n",
      "\u001b[1;36m(EngineCore_DP0 pid=103695)\u001b[0;0m ERROR 11-27 23:04:35 [core.py:842]   File \"/home/jianing/anaconda3/envs/kag2/lib/python3.12/site-packages/vllm/v1/engine/core.py\", line 102, in __init__\n",
      "\u001b[1;36m(EngineCore_DP0 pid=103695)\u001b[0;0m ERROR 11-27 23:04:35 [core.py:842]     self.model_executor = executor_class(vllm_config)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=103695)\u001b[0;0m ERROR 11-27 23:04:35 [core.py:842]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(EngineCore_DP0 pid=103695)\u001b[0;0m ERROR 11-27 23:04:35 [core.py:842]   File \"/home/jianing/anaconda3/envs/kag2/lib/python3.12/site-packages/vllm/v1/executor/abstract.py\", line 101, in __init__\n",
      "\u001b[1;36m(EngineCore_DP0 pid=103695)\u001b[0;0m ERROR 11-27 23:04:35 [core.py:842]     self._init_executor()\n",
      "\u001b[1;36m(EngineCore_DP0 pid=103695)\u001b[0;0m ERROR 11-27 23:04:35 [core.py:842]   File \"/home/jianing/anaconda3/envs/kag2/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py\", line 47, in _init_executor\n",
      "\u001b[1;36m(EngineCore_DP0 pid=103695)\u001b[0;0m ERROR 11-27 23:04:35 [core.py:842]     self.driver_worker.init_device()\n",
      "\u001b[1;36m(EngineCore_DP0 pid=103695)\u001b[0;0m ERROR 11-27 23:04:35 [core.py:842]   File \"/home/jianing/anaconda3/envs/kag2/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py\", line 324, in init_device\n",
      "\u001b[1;36m(EngineCore_DP0 pid=103695)\u001b[0;0m ERROR 11-27 23:04:35 [core.py:842]     self.worker.init_device()  # type: ignore\n",
      "\u001b[1;36m(EngineCore_DP0 pid=103695)\u001b[0;0m ERROR 11-27 23:04:35 [core.py:842]     ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(EngineCore_DP0 pid=103695)\u001b[0;0m ERROR 11-27 23:04:35 [core.py:842]   File \"/home/jianing/anaconda3/envs/kag2/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py\", line 247, in init_device\n",
      "\u001b[1;36m(EngineCore_DP0 pid=103695)\u001b[0;0m ERROR 11-27 23:04:35 [core.py:842]     raise ValueError(\n",
      "\u001b[1;36m(EngineCore_DP0 pid=103695)\u001b[0;0m ERROR 11-27 23:04:35 [core.py:842] ValueError: Free memory on device (10.3/11.99 GiB) on startup is less than desired GPU memory utilization (0.9, 10.79 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=103695)\u001b[0;0m Process EngineCore_DP0:\n",
      "\u001b[1;36m(EngineCore_DP0 pid=103695)\u001b[0;0m Traceback (most recent call last):\n",
      "\u001b[1;36m(EngineCore_DP0 pid=103695)\u001b[0;0m   File \"/home/jianing/anaconda3/envs/kag2/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "\u001b[1;36m(EngineCore_DP0 pid=103695)\u001b[0;0m     self.run()\n",
      "\u001b[1;36m(EngineCore_DP0 pid=103695)\u001b[0;0m   File \"/home/jianing/anaconda3/envs/kag2/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "\u001b[1;36m(EngineCore_DP0 pid=103695)\u001b[0;0m     self._target(*self._args, **self._kwargs)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=103695)\u001b[0;0m   File \"/home/jianing/anaconda3/envs/kag2/lib/python3.12/site-packages/vllm/v1/engine/core.py\", line 846, in run_engine_core\n",
      "\u001b[1;36m(EngineCore_DP0 pid=103695)\u001b[0;0m     raise e\n",
      "\u001b[1;36m(EngineCore_DP0 pid=103695)\u001b[0;0m   File \"/home/jianing/anaconda3/envs/kag2/lib/python3.12/site-packages/vllm/v1/engine/core.py\", line 833, in run_engine_core\n",
      "\u001b[1;36m(EngineCore_DP0 pid=103695)\u001b[0;0m     engine_core = EngineCoreProc(*args, **kwargs)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=103695)\u001b[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(EngineCore_DP0 pid=103695)\u001b[0;0m   File \"/home/jianing/anaconda3/envs/kag2/lib/python3.12/site-packages/vllm/v1/engine/core.py\", line 606, in __init__\n",
      "\u001b[1;36m(EngineCore_DP0 pid=103695)\u001b[0;0m     super().__init__(\n",
      "\u001b[1;36m(EngineCore_DP0 pid=103695)\u001b[0;0m   File \"/home/jianing/anaconda3/envs/kag2/lib/python3.12/site-packages/vllm/v1/engine/core.py\", line 102, in __init__\n",
      "\u001b[1;36m(EngineCore_DP0 pid=103695)\u001b[0;0m     self.model_executor = executor_class(vllm_config)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=103695)\u001b[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(EngineCore_DP0 pid=103695)\u001b[0;0m   File \"/home/jianing/anaconda3/envs/kag2/lib/python3.12/site-packages/vllm/v1/executor/abstract.py\", line 101, in __init__\n",
      "\u001b[1;36m(EngineCore_DP0 pid=103695)\u001b[0;0m     self._init_executor()\n",
      "\u001b[1;36m(EngineCore_DP0 pid=103695)\u001b[0;0m   File \"/home/jianing/anaconda3/envs/kag2/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py\", line 47, in _init_executor\n",
      "\u001b[1;36m(EngineCore_DP0 pid=103695)\u001b[0;0m     self.driver_worker.init_device()\n",
      "\u001b[1;36m(EngineCore_DP0 pid=103695)\u001b[0;0m   File \"/home/jianing/anaconda3/envs/kag2/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py\", line 324, in init_device\n",
      "\u001b[1;36m(EngineCore_DP0 pid=103695)\u001b[0;0m     self.worker.init_device()  # type: ignore\n",
      "\u001b[1;36m(EngineCore_DP0 pid=103695)\u001b[0;0m     ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(EngineCore_DP0 pid=103695)\u001b[0;0m   File \"/home/jianing/anaconda3/envs/kag2/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py\", line 247, in init_device\n",
      "\u001b[1;36m(EngineCore_DP0 pid=103695)\u001b[0;0m     raise ValueError(\n",
      "\u001b[1;36m(EngineCore_DP0 pid=103695)\u001b[0;0m ValueError: Free memory on device (10.3/11.99 GiB) on startup is less than desired GPU memory utilization (0.9, 10.79 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Engine core initialization failed. See root cause above. Failed core proc(s): {}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[37]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      9\u001b[39m MAX_NUM_SEQS = \u001b[32m2\u001b[39m\n\u001b[32m     10\u001b[39m MAX_MODEL_LEN = \u001b[32m32768\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m llm = \u001b[43mLLM\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43mllm_model_pth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mbfloat16\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m                \u001b[49m\u001b[38;5;66;43;03m# The data type for the model weights and activations\u001b[39;49;00m\n\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_num_seqs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mMAX_NUM_SEQS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m   \u001b[49m\u001b[38;5;66;43;03m# Maximum number of sequences per iteration. Default is 256\u001b[39;49;00m\n\u001b[32m     16\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_model_len\u001b[49m\u001b[43m=\u001b[49m\u001b[43mMAX_MODEL_LEN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Model context length\u001b[39;49;00m\n\u001b[32m     17\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m      \u001b[49m\u001b[38;5;66;43;03m# Trust remote code (e.g., from HuggingFace) when downloading the model and tokenizer\u001b[39;49;00m\n\u001b[32m     18\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensor_parallel_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m      \u001b[49m\u001b[38;5;66;43;03m# The number of GPUs to use for distributed execution with tensor parallelism\u001b[39;49;00m\n\u001b[32m     19\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgpu_memory_utilization\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.9\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# The ratio (between 0 and 1) of GPU memory to reserve for the model\u001b[39;49;00m\n\u001b[32m     20\u001b[39m \u001b[43m    \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m391\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m    \u001b[49m\u001b[43menable_prefix_caching\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/kag2/lib/python3.12/site-packages/vllm/entrypoints/llm.py:343\u001b[39m, in \u001b[36mLLM.__init__\u001b[39m\u001b[34m(self, model, runner, convert, tokenizer, tokenizer_mode, skip_tokenizer_init, trust_remote_code, allowed_local_media_path, allowed_media_domains, tensor_parallel_size, dtype, quantization, revision, tokenizer_revision, seed, gpu_memory_utilization, swap_space, cpu_offload_gb, enforce_eager, disable_custom_all_reduce, hf_token, hf_overrides, mm_processor_kwargs, pooler_config, override_pooler_config, structured_outputs_config, kv_cache_memory_bytes, compilation_config, logits_processors, **kwargs)\u001b[39m\n\u001b[32m    340\u001b[39m log_non_default_args(engine_args)\n\u001b[32m    342\u001b[39m \u001b[38;5;66;03m# Create the Engine (autoselects V0 vs V1)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m343\u001b[39m \u001b[38;5;28mself\u001b[39m.llm_engine = \u001b[43mLLMEngine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_engine_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    344\u001b[39m \u001b[43m    \u001b[49m\u001b[43mengine_args\u001b[49m\u001b[43m=\u001b[49m\u001b[43mengine_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43musage_context\u001b[49m\u001b[43m=\u001b[49m\u001b[43mUsageContext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mLLM_CLASS\u001b[49m\n\u001b[32m    345\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    346\u001b[39m \u001b[38;5;28mself\u001b[39m.engine_class = \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m.llm_engine)\n\u001b[32m    348\u001b[39m \u001b[38;5;28mself\u001b[39m.request_counter = Counter()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/kag2/lib/python3.12/site-packages/vllm/v1/engine/llm_engine.py:174\u001b[39m, in \u001b[36mLLMEngine.from_engine_args\u001b[39m\u001b[34m(cls, engine_args, usage_context, stat_loggers, enable_multiprocessing)\u001b[39m\n\u001b[32m    171\u001b[39m     enable_multiprocessing = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    173\u001b[39m \u001b[38;5;66;03m# Create the LLMEngine.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m174\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    175\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    176\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexecutor_class\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexecutor_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    177\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mengine_args\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdisable_log_stats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    178\u001b[39m \u001b[43m    \u001b[49m\u001b[43musage_context\u001b[49m\u001b[43m=\u001b[49m\u001b[43musage_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    179\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstat_loggers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstat_loggers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    180\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmultiprocess_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43menable_multiprocessing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    181\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/kag2/lib/python3.12/site-packages/vllm/v1/engine/llm_engine.py:108\u001b[39m, in \u001b[36mLLMEngine.__init__\u001b[39m\u001b[34m(self, vllm_config, executor_class, log_stats, aggregate_engine_logging, usage_context, stat_loggers, mm_registry, use_cached_outputs, multiprocess_mode)\u001b[39m\n\u001b[32m    105\u001b[39m     \u001b[38;5;28mself\u001b[39m.output_processor.tracer = tracer\n\u001b[32m    107\u001b[39m \u001b[38;5;66;03m# EngineCore (gets EngineCoreRequests and gives EngineCoreOutputs)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m108\u001b[39m \u001b[38;5;28mself\u001b[39m.engine_core = \u001b[43mEngineCoreClient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmake_client\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    109\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmultiprocess_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmultiprocess_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    110\u001b[39m \u001b[43m    \u001b[49m\u001b[43masyncio_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    111\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    112\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexecutor_class\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexecutor_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    113\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    114\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    116\u001b[39m \u001b[38;5;28mself\u001b[39m.logger_manager: StatLoggerManager | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    117\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.log_stats:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/kag2/lib/python3.12/site-packages/vllm/v1/engine/core_client.py:93\u001b[39m, in \u001b[36mEngineCoreClient.make_client\u001b[39m\u001b[34m(multiprocess_mode, asyncio_mode, vllm_config, executor_class, log_stats)\u001b[39m\n\u001b[32m     88\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m EngineCoreClient.make_async_mp_client(\n\u001b[32m     89\u001b[39m         vllm_config, executor_class, log_stats\n\u001b[32m     90\u001b[39m     )\n\u001b[32m     92\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m multiprocess_mode \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m asyncio_mode:\n\u001b[32m---> \u001b[39m\u001b[32m93\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSyncMPClient\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexecutor_class\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     95\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m InprocClient(vllm_config, executor_class, log_stats)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/kag2/lib/python3.12/site-packages/vllm/v1/engine/core_client.py:640\u001b[39m, in \u001b[36mSyncMPClient.__init__\u001b[39m\u001b[34m(self, vllm_config, executor_class, log_stats)\u001b[39m\n\u001b[32m    637\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\n\u001b[32m    638\u001b[39m     \u001b[38;5;28mself\u001b[39m, vllm_config: VllmConfig, executor_class: \u001b[38;5;28mtype\u001b[39m[Executor], log_stats: \u001b[38;5;28mbool\u001b[39m\n\u001b[32m    639\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m640\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    641\u001b[39m \u001b[43m        \u001b[49m\u001b[43masyncio_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    642\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    643\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexecutor_class\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexecutor_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    644\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    645\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    647\u001b[39m     \u001b[38;5;28mself\u001b[39m.is_dp = \u001b[38;5;28mself\u001b[39m.vllm_config.parallel_config.data_parallel_size > \u001b[32m1\u001b[39m\n\u001b[32m    648\u001b[39m     \u001b[38;5;28mself\u001b[39m.outputs_queue = queue.Queue[EngineCoreOutputs | \u001b[38;5;167;01mException\u001b[39;00m]()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/kag2/lib/python3.12/site-packages/vllm/v1/engine/core_client.py:469\u001b[39m, in \u001b[36mMPClient.__init__\u001b[39m\u001b[34m(self, asyncio_mode, vllm_config, executor_class, log_stats, client_addresses)\u001b[39m\n\u001b[32m    466\u001b[39m     \u001b[38;5;28mself\u001b[39m.stats_update_address = client_addresses.get(\u001b[33m\"\u001b[39m\u001b[33mstats_update_address\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    467\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    468\u001b[39m     \u001b[38;5;66;03m# Engines are managed by this client.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m469\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mlaunch_core_engines\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexecutor_class\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mas\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    470\u001b[39m \u001b[43m        \u001b[49m\u001b[43mengine_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    471\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcoordinator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    472\u001b[39m \u001b[43m        \u001b[49m\u001b[43maddresses\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    473\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    474\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mresources\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcoordinator\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mcoordinator\u001b[49m\n\u001b[32m    475\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mresources\u001b[49m\u001b[43m.\u001b[49m\u001b[43mengine_manager\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mengine_manager\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/kag2/lib/python3.12/contextlib.py:144\u001b[39m, in \u001b[36m_GeneratorContextManager.__exit__\u001b[39m\u001b[34m(self, typ, value, traceback)\u001b[39m\n\u001b[32m    142\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m typ \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    143\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m144\u001b[39m         \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    145\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[32m    146\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/kag2/lib/python3.12/site-packages/vllm/v1/engine/utils.py:907\u001b[39m, in \u001b[36mlaunch_core_engines\u001b[39m\u001b[34m(vllm_config, executor_class, log_stats, num_api_servers)\u001b[39m\n\u001b[32m    904\u001b[39m \u001b[38;5;28;01myield\u001b[39;00m local_engine_manager, coordinator, addresses\n\u001b[32m    906\u001b[39m \u001b[38;5;66;03m# Now wait for engines to start.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m907\u001b[39m \u001b[43mwait_for_engine_startup\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    908\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhandshake_socket\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    909\u001b[39m \u001b[43m    \u001b[49m\u001b[43maddresses\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    910\u001b[39m \u001b[43m    \u001b[49m\u001b[43mengines_to_handshake\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    911\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparallel_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    912\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcache_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    913\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlocal_engine_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    914\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcoordinator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mproc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcoordinator\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    915\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/kag2/lib/python3.12/site-packages/vllm/v1/engine/utils.py:964\u001b[39m, in \u001b[36mwait_for_engine_startup\u001b[39m\u001b[34m(handshake_socket, addresses, core_engines, parallel_config, cache_config, proc_manager, coord_process)\u001b[39m\n\u001b[32m    962\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m coord_process \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m coord_process.exitcode \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    963\u001b[39m         finished[coord_process.name] = coord_process.exitcode\n\u001b[32m--> \u001b[39m\u001b[32m964\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    965\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mEngine core initialization failed. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    966\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mSee root cause above. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    967\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFailed core proc(s): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfinished\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    968\u001b[39m     )\n\u001b[32m    970\u001b[39m \u001b[38;5;66;03m# Receive HELLO and READY messages from the input socket.\u001b[39;00m\n\u001b[32m    971\u001b[39m eng_identity, ready_msg_bytes = handshake_socket.recv_multipart()\n",
      "\u001b[31mRuntimeError\u001b[39m: Engine core initialization failed. See root cause above. Failed core proc(s): {}"
     ]
    }
   ],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "llm_model_pth = '/mnt/d/Qwen3-1.7B'\n",
    "\n",
    "MAX_NUM_SEQS = 2\n",
    "MAX_MODEL_LEN = 32768\n",
    "\n",
    "llm = LLM(\n",
    "    llm_model_pth,\n",
    "    dtype=\"bfloat16\",                # The data type for the model weights and activations\n",
    "    max_num_seqs=MAX_NUM_SEQS,   # Maximum number of sequences per iteration. Default is 256\n",
    "    max_model_len=MAX_MODEL_LEN, # Model context length\n",
    "    trust_remote_code=False,      # Trust remote code (e.g., from HuggingFace) when downloading the model and tokenizer\n",
    "    tensor_parallel_size=1,      # The number of GPUs to use for distributed execution with tensor parallelism\n",
    "    gpu_memory_utilization=0.9, # The ratio (between 0 and 1) of GPU memory to reserve for the model\n",
    "    seed=391,\n",
    "    enable_prefix_caching=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.execute_input": "2025-11-24T04:45:11.220809Z",
     "iopub.status.busy": "2025-11-24T04:45:11.220327Z",
     "iopub.status.idle": "2025-11-24T04:45:11.226095Z",
     "shell.execute_reply": "2025-11-24T04:45:11.225514Z",
     "shell.execute_reply.started": "2025-11-24T04:45:11.220793Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.11.2\n"
     ]
    }
   ],
   "source": [
    "import vllm\n",
    "print(vllm.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "tokenizer = llm.get_tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import keyword\n",
    "\n",
    "\n",
    "def extract_boxed_text(text):\n",
    "    pattern = r'oxed{(.*?)}'\n",
    "    matches = re.findall(pattern, text)\n",
    "    if not matches:\n",
    "        return \"\"\n",
    "    for match in matches[::-1]:\n",
    "        if match != \"\":\n",
    "            return match\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "from collections import Counter\n",
    "import random\n",
    "def select_answer(answers):\n",
    "    counter = Counter()\n",
    "    for answer in answers:\n",
    "        try:\n",
    "            if int(answer) == float(answer):\n",
    "                counter[int(answer)] += 1 + random.random() / 1_000\n",
    "        except:\n",
    "            pass\n",
    "    if not counter:\n",
    "        return 3\n",
    "    _, answer = sorted([(v,k) for k,v in counter.items()], reverse=True)[0]\n",
    "    return answer%10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def create_starter_messages(question, index):\n",
    "    options = []\n",
    "    for _ in range(3):\n",
    "        options.append(\n",
    "            [\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful and harmless assistant. You are Qwen developed by Alibaba. You should think step-by-step.\"},\n",
    "                {\"role\": \"user\", \"content\": question + ' Return final answer within \\boxed{}, after taking modulo 1000.'},\n",
    "            ]\n",
    "        )\n",
    "    for _ in range(2):\n",
    "        options.append(\n",
    "            [\n",
    "                {\"role\": \"system\", \"content\": \"You are a the most powerful math expert. Please solve the problems with deep resoning. You are careful and always recheck your conduction. You will never give answer directly until you have enough confidence. You should think step-by-step. Return final answer within \\\\boxed{}, after taking modulo 1000.\"},\n",
    "                {\"role\": \"user\", \"content\": question},\n",
    "            ]\n",
    "        )\n",
    "    for _ in range(1):    \n",
    "        options.append(\n",
    "            [\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful and harmless math assistant. You should think step-by-step and you are good at reverse thinking to recheck your answer and fix all possible mistakes. After you get your final answer, take modulo 1000, and return the final answer within \\\\boxed{}.\"},\n",
    "                {\"role\": \"user\", \"content\": question},\n",
    "            ],\n",
    "        )\n",
    "\n",
    "    options.append(\n",
    "        [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": (\n",
    "                    \"You are a mathematical genius. \"\n",
    "                    \"Explain each step clearly and verify for consistency. \"\n",
    "                    \"Give your final solution in \\\\boxed{} with the result mod 1000.\"\n",
    "                )\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": question\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # 11\n",
    "    options.append(\n",
    "        [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": (\n",
    "                    \"You are a highly accurate math assistant. \"\n",
    "                    \"Pay special attention to possible pitfalls. \"\n",
    "                    \"Your final answer must be in \\\\boxed{}, and taken modulo 1000.\"\n",
    "                )\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": question\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "    return options[index%len(options)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from math import ceil\n",
    "\n",
    "def predict_for_question(question: str) -> int:\n",
    "    question_start_time = time.time()\n",
    "    if time.time() > cutoff_time:\n",
    "        return 3\n",
    "    selected_questions_only = True\n",
    "    # if selected_questions_only and not os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n",
    "    #     # if \"Triangle\" not in question:\n",
    "    #     return 210\n",
    "    print(question)\n",
    "    \n",
    "    if time.time() > cutoff_times[-1]:\n",
    "        print(time.time(), cutoff_times[-1])\n",
    "        print(\"SPEEDUP!!!\")\n",
    "        num_of_iters = 2\n",
    "        num_of_seqs=6\n",
    "    else:\n",
    "        num_of_iters = 3\n",
    "        num_of_seqs=6\n",
    "\n",
    "\n",
    "    messages = [create_starter_messages(question, index) for index in range(num_of_seqs)]\n",
    "    list_of_texts = [\n",
    "        tokenizer.apply_chat_template(\n",
    "            conversation=message,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True\n",
    "        )\n",
    "        for message in messages\n",
    "    ]\n",
    "    old = list_of_texts\n",
    "    new = list_of_texts\n",
    "    res = []\n",
    "\n",
    "    for k in range(num_of_iters):\n",
    "        # if k == 1:\n",
    "        sampling_params = SamplingParams(\n",
    "            temperature=0.7,              # randomness of the sampling\n",
    "            min_p=0.05,\n",
    "            top_p=0.9,\n",
    "            skip_special_tokens=True,     # Whether to skip special tokens in the output\n",
    "            max_tokens=32768 ,\n",
    "            # stop=[\"</think>\"]\n",
    "        )\n",
    "        outputs = llm.generate(old,  sampling_params=sampling_params)\n",
    "        print(outputs[0].outputs[0].text)\n",
    "        new = []\n",
    "\n",
    "        new_old = []\n",
    "        for i in range(len(old)):\n",
    "            if extract_boxed_text(outputs[i].outputs[0].text):\n",
    "                res.append(old[i] + outputs[i].outputs[0].text)\n",
    "            else:\n",
    "                new_old.append(old[i] + outputs[i].outputs[0].text)       \n",
    "\n",
    "        if num_of_iters == 3 and k == 1 and len(new_old) >= 7:\n",
    "            curr_num = min(7, len(new_old))\n",
    "            \n",
    "            new_old = random.sample(new_old, curr_num)\n",
    "        print(list(extract_boxed_text(x) for x in res))\n",
    "        if k == 0:\n",
    "            repeats = 2\n",
    "        elif k == 1:\n",
    "            repeats = 1\n",
    "        for i in range(len(new_old)):\n",
    "            for _ in range(repeats):\n",
    "                new.append(new_old[i])\n",
    "        old = new\n",
    "\n",
    "        print(list(extract_boxed_text(x) for x in res))\n",
    "\n",
    "        if len(res) >= 8:\n",
    "            numbers = []\n",
    "            for r in res:\n",
    "                try:\n",
    "                    num = int(extract_boxed_text(r))\n",
    "                    numbers.append(num)\n",
    "                except ValueError:\n",
    "                    pass\n",
    "            # Если удалось собрать 7+ числовых ответов\n",
    "            if len(numbers) >= 8:\n",
    "                counter = Counter(numbers)\n",
    "                total = len(numbers)\n",
    "                \n",
    "                for num, cnt in counter.items():\n",
    "                    if cnt >= ceil(total * 0.8):\n",
    "                        cutoff_times.pop()\n",
    "                        question_end_time = time.time()\n",
    "                        print(num)\n",
    "                        print(\"TIME FOR QUESTION: \", question_end_time - question_start_time)\n",
    "                        print(\"PRE RESULT!!!\")\n",
    "                        return num\n",
    "            \n",
    "    answer = select_answer(list(extract_boxed_text(x) for x in res))\n",
    "    print(answer)\n",
    "    cutoff_times.pop()\n",
    "    question_end_time = time.time()\n",
    "    print(\"TIME FOR QUESTION: \", question_end_time - question_start_time)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Replace this function with your inference code.\n",
    "# The function should return a single integer between 0 and 999, inclusive.\n",
    "# Each prediction (except the very first) must be returned within 30 minutes of the question being provided.\n",
    "def predict(id_: pl.DataFrame, question: pl.DataFrame) -> pl.DataFrame:\n",
    "    id_ = id_\n",
    "    # print(\"------\")\n",
    "    # print(id_)\n",
    "    question_str = question\n",
    "    # print(question_str)\n",
    "    answer = predict_for_question(question_str)\n",
    "    # print(\"------\\n\\n\")\n",
    "    return pl.DataFrame({'id': id_, 'answer': answer})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "problems = pd.read_csv('/home/jianing/nlp/aimo3/ai-mathematical-olympiad-progress-prize-3/test.csv'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "cutoff_times = [int(x) for x in np.linspace(cutoff_time, time.time() + 500, 50)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (1, 1)\n",
      "┌────────────────┐\n",
      "│                │\n",
      "│ ---            │\n",
      "│ str            │\n",
      "╞════════════════╡\n",
      "│ What is $1-1$? │\n",
      "└────────────────┘\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests: 100%|██████████| 6/6 [00:00<00:00, 918.26it/s]\n",
      "Processed prompts: 100%|██████████| 6/6 [00:17<00:00,  2.99s/it, est. speed input: 18.48 toks/s, output: 150.23 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2', '2']\n",
      "['2', '2']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests: 100%|██████████| 8/8 [00:00<00:00, 1343.57it/s]\n",
      "Processed prompts: 100%|██████████| 8/8 [00:00<00:00, 86.10it/s, est. speed input: 15603.29 toks/s, output: 108.34 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2', '2']\n",
      "['2', '2']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests: 100%|██████████| 7/7 [00:00<00:00, 1807.78it/s]\n",
      "Processed prompts: 100%|██████████| 7/7 [00:00<00:00, 20.83it/s, est. speed input: 3702.75 toks/s, output: 101.44 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2', '2']\n",
      "['2', '2']\n",
      "2\n",
      "TIME FOR QUESTION:  18.431315660476685\n",
      "shape: (1, 1)\n",
      "┌──────────────────────┐\n",
      "│                      │\n",
      "│ ---                  │\n",
      "│ str                  │\n",
      "╞══════════════════════╡\n",
      "│ What is $0\\times10$? │\n",
      "└──────────────────────┘\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests: 100%|██████████| 6/6 [00:00<00:00, 2672.38it/s]\n",
      "Processed prompts: 100%|██████████| 6/6 [01:05<00:00, 10.97s/it, est. speed input: 5.05 toks/s, output: 137.00 toks/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['376', '376']\n",
      "['376', '376']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests: 100%|██████████| 8/8 [00:00<00:00, 1703.70it/s]\n",
      "Processed prompts: 100%|██████████| 8/8 [00:00<00:00, 79.58it/s, est. speed input: 12644.65 toks/s, output: 80.02 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['376', '376']\n",
      "['376', '376']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests: 100%|██████████| 7/7 [00:00<00:00, 1854.95it/s]\n",
      "Processed prompts: 100%|██████████| 7/7 [00:00<00:00, 89.88it/s, est. speed input: 13886.80 toks/s, output: 90.59 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['376', '376']\n",
      "['376', '376']\n",
      "376\n",
      "TIME FOR QUESTION:  65.99251341819763\n",
      "shape: (1, 1)\n",
      "┌────────────────────────┐\n",
      "│                        │\n",
      "│ ---                    │\n",
      "│ str                    │\n",
      "╞════════════════════════╡\n",
      "│ Solve $4+x=4$ for $x$. │\n",
      "└────────────────────────┘\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests: 100%|██████████| 6/6 [00:00<00:00, 2744.07it/s]\n",
      "Processed prompts: 100%|██████████| 6/6 [05:08<00:00, 51.50s/it, est. speed input: 1.07 toks/s, output: 73.98 toks/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['376']\n",
      "['376']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests: 100%|██████████| 10/10 [00:00<00:00, 1148.21it/s]\n",
      "Processed prompts: 100%|██████████| 10/10 [00:00<00:00, 97.41it/s, est. speed input: 17882.38 toks/s, output: 98.46 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['376']\n",
      "['376']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests: 100%|██████████| 7/7 [00:00<00:00, 1812.02it/s]\n",
      "Processed prompts: 100%|██████████| 7/7 [00:00<00:00, 121.50it/s, est. speed input: 20285.89 toks/s, output: 122.72 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['376']\n",
      "['376']\n",
      "376\n",
      "TIME FOR QUESTION:  309.16942524909973\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "submissions = []\n",
    "for index, problem_row in problems.iterrows():\n",
    "    \n",
    "    id_value = problem_row['id']\n",
    "    question_value = problem_row['problem']\n",
    "    \n",
    "    # Pass a single-column Polars Series instead of a DataFrame created from a dictionary.\n",
    "    # This might allow the internal .item(0) to correctly interpret the index 0 as the row index.\n",
    "    id_pl_df = pl.Series([id_value]).to_frame()\n",
    "    question_pl_df = pl.Series([question_value]).to_frame()\n",
    "    \n",
    "    # The predict function expects two positional arguments, so we pass the two resulting DataFrames\n",
    "    submission_result = predict(id_pl_df, question_pl_df)\n",
    "    \n",
    "    submissions.append(submission_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[shape: (1, 2)\n",
       " ┌────────┬────────┐\n",
       " │ id     ┆ answer │\n",
       " │ ---    ┆ ---    │\n",
       " │ str    ┆ i32    │\n",
       " ╞════════╪════════╡\n",
       " │ 000aaa ┆ 2      │\n",
       " └────────┴────────┘,\n",
       " shape: (1, 2)\n",
       " ┌────────┬────────┐\n",
       " │ id     ┆ answer │\n",
       " │ ---    ┆ ---    │\n",
       " │ str    ┆ i32    │\n",
       " ╞════════╪════════╡\n",
       " │ 111bbb ┆ 376    │\n",
       " └────────┴────────┘,\n",
       " shape: (1, 2)\n",
       " ┌────────┬────────┐\n",
       " │ id     ┆ answer │\n",
       " │ ---    ┆ ---    │\n",
       " │ str    ┆ i32    │\n",
       " ╞════════╪════════╡\n",
       " │ 222ccc ┆ 376    │\n",
       " └────────┴────────┘]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pl.concat(submissions)\n",
    "df.write_csv('/home/jianing/nlp/aimo3/ai-mathematical-olympiad-progress-prize-3/submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 14559231,
     "sourceId": 118448,
     "sourceType": "competition"
    },
    {
     "sourceId": 263213121,
     "sourceType": "kernelVersion"
    },
    {
     "modelId": 224071,
     "modelInstanceId": 202436,
     "sourceId": 237029,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "kag2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
